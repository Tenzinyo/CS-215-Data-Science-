{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265d053d",
   "metadata": {},
   "source": [
    "# In-Class Activity: Getting Data from APIs\n",
    "\n",
    "Today we will learn how to:\n",
    "* Use requests to access an API\n",
    "* Parse the data we get from an API (in JSON)\n",
    "* Retrieve historical web data from the Internet Archive\n",
    "* Play around with the TV Maze API\n",
    "\n",
    "A very big thank you to Brian Keegan (my advisor!) and the materials in his [Web Data Scraping course](https://github.com/CU-ITSS/Web-Data-Scraping-S2023). Check that out if you want to dig deeper ;) Another thank you to Jason Zeitz and Anas Buhayh at University of Colorado, Boulder, who developed some of the TV Maze API content.\n",
    "\n",
    "But first... a warm-up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c3771",
   "metadata": {},
   "source": [
    "### Refresher: Lists and Dictionaries in Python\n",
    "Often we get data in the forms of lists. Lists are an **ordered** data structure that can contain integers, strings, or other objects (like lists or dictionaries), Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "566a9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make classrooms as lists with student names as strings\n",
    "classroom0 = ['Alice','Bob','Carol','Dave']\n",
    "classroom1 = ['Eve','Frank','Grace','Harold']\n",
    "classroom2 = ['Isabel','Jack','Katy','Lloyd']\n",
    "classroom3 = ['Maria','Nate','Olivia','Philip']\n",
    "classroom4 = ['Quinn','Rachel','Steve','Terry','Ursula']\n",
    "classroom5 = ['Violet','Walter','Xavier','Yves','Zoe']\n",
    "\n",
    "# Make schools that contain classrooms\n",
    "school0 = [classroom0,classroom1]\n",
    "school1 = [classroom2,classroom3]\n",
    "school2 = [classroom4,classroom5]\n",
    "\n",
    "# Make a school district that contains schools\n",
    "school_district = [school0,school1,school2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17c21e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'Bob', 'Carol', 'Dave']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1: How would you access classroom0 **FROM** school_district, using index notation?\n",
    "school_district[0][0]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488e259f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maria'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2: How would you access the 0th student in classroom3 (Maria) **FROM** school_district, using index notation?\n",
    "school_district[1][1][0]\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cf982",
   "metadata": {},
   "source": [
    "We also often get data in the form of dictionaries. Dictionaries are an **unordered** data structure containing key-value pairs, kind of like like a phonebook.\n",
    "\n",
    "Here's a dictionary with information about the states in the Pacific Northwest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905852dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_northwest = {\n",
    "    'Washington' : {\n",
    "        'Abbreviation': 'WA',\n",
    "        'Area': 71362,\n",
    "        'Capital': 'Olympia',\n",
    "        'Established': '1889-11-11',\n",
    "        'Largest city': 'Seattle',\n",
    "        'Population': 7887965,\n",
    "        'Representatives': 10\n",
    "    },\n",
    "    'Idaho': {\n",
    "        'Abbreviation': 'ID',\n",
    "        'Area': 83569,\n",
    "        'Capital': 'Boise',\n",
    "        'Established': '1890-07-03',\n",
    "        'Largest city': 'Boise',\n",
    "        'Population': 1839106,\n",
    "        'Representatives': 2\n",
    "    },\n",
    "    'Oregon': {\n",
    "        'Abbreviation': 'OR',\n",
    "        'Area': 98381,\n",
    "        'Capital': 'Salem',\n",
    "        'Established': '1859-02-14',\n",
    "        'Largest city': 'Portland',\n",
    "        'Population': 4246155,\n",
    "        'Representatives': 6\n",
    "    }\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c49dc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Washington', 'Idaho', 'Oregon'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: How would you list get of the keys in this dictionary?\n",
    "pacific_northwest.keys()\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d8ab6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([{'Abbreviation': 'WA', 'Area': 71362, 'Capital': 'Olympia', 'Established': '1889-11-11', 'Largest city': 'Seattle', 'Population': 7887965, 'Representatives': 10}, {'Abbreviation': 'ID', 'Area': 83569, 'Capital': 'Boise', 'Established': '1890-07-03', 'Largest city': 'Boise', 'Population': 1839106, 'Representatives': 2}, {'Abbreviation': 'OR', 'Area': 98381, 'Capital': 'Salem', 'Established': '1859-02-14', 'Largest city': 'Portland', 'Population': 4246155, 'Representatives': 6}])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: How would you access all of the values?\n",
    "pacific_northwest.values()\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bd022c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abbreviation': 'WA',\n",
       " 'Area': 71362,\n",
       " 'Capital': 'Olympia',\n",
       " 'Established': '1889-11-11',\n",
       " 'Largest city': 'Seattle',\n",
       " 'Population': 7887965,\n",
       " 'Representatives': 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: How would you access *all* of the information about Washington?\n",
    "pacific_northwest['Washington']\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "802e8353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4246155"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: How would you access the population of Oregon?\n",
    "pacific_northwest['Oregon']['Population']\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31f9c9",
   "metadata": {},
   "source": [
    "Nested data structures do not need to be the same data type. Here's the same information above, but as a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa09499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_northwest_list = [\n",
    "    {'Name': 'Washington',\n",
    "     'Abbreviation': 'WA',\n",
    "     'Area': 71362,\n",
    "     'Capital': 'Olympia',\n",
    "     'Established': '1889-11-11',\n",
    "     'Largest city': 'Seattle',\n",
    "     'Population': 7887965,\n",
    "     'Representatives': 10\n",
    "    },\n",
    "    {'Name':'Idaho',\n",
    "     'Abbreviation': 'ID',\n",
    "     'Area': 83569,\n",
    "     'Capital': 'Boise',\n",
    "     'Established': '1890-07-03',\n",
    "     'Largest city': 'Boise',\n",
    "     'Population': 1839106,\n",
    "     'Representatives': 2\n",
    "    },\n",
    "    {'Name': 'Oregon',\n",
    "     'Abbreviation': 'OR',\n",
    "     'Area': 98381,\n",
    "     'Capital': 'Salem',\n",
    "     'Established': '1859-02-14',\n",
    "     'Largest city': 'Portland',\n",
    "     'Population': 4246155,\n",
    "     'Representatives': 6\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c81fbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: How would you access the capital of Idaho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86197b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boise'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pacific_northwest_list[1]['Capital']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "563d6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington:7887965\n",
      "Idaho:1839106\n",
      "Oregon:4246155\n"
     ]
    }
   ],
   "source": [
    "# Task: How would you print out all of the state names and populations?\n",
    "for state in pacific_northwest_list:\n",
    "    name=state['Name']\n",
    "    pop=state['Population']\n",
    "    print(name+':'+ str(pop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c149ff",
   "metadata": {},
   "source": [
    "Ok, now we are ready to access APIs, which often return data in the JSON format. [JavaScript Object Notation (JSON)](https://www.json.org/) is probably the most popular data markup language and is especially ubiquitous when retreiving data from the application programming interfaces (APIs) of popular platforms like Twitter, Reddit, Wikipedia, etc.\n",
    "\n",
    "JSON is attractive for programmers using JavaScript and Python because it can represent a mix of different data types.\n",
    "\n",
    "What you need to know is that JSON is very similar to the form of a Python dictionary, and it can contain other data structures (for example, lists).\n",
    "\n",
    "_**As you start to work with APIs, remember to always put on your data detective hats and figure out what structure you are in and how to extract information from it!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269abbac",
   "metadata": {},
   "source": [
    "### Getting historical web pages from the Wayback Machine API\n",
    "\n",
    "Now we are ready to start using APIs! For our first example, we'll use the Wayback Machine, a service from the [Internet Archive](https://archive.org/), which is a database of historical webpages and media content.\n",
    "\n",
    "For fun, let's look at a few:\n",
    "* [CNN in June 2000](https://web.archive.org/web/20000815052826/http://www.cnn.com/)\n",
    "* [Apple in April 1997](https://web.archive.org/web/19970404064444/http://www.apple.com:80/)\n",
    "* [Whitman College in 2002](https://web.archive.org/web/20020124214454/http://www.whitman.edu/)\n",
    "\n",
    "\n",
    "---\n",
    "**Activity:**\n",
    "\n",
    "Visit the Wayback Machine at [https://web.archive.org](https://web.archive.org/) and check out a historical version of a page that interests you. Share it with your partner.\n",
    "\n",
    "---\n",
    "\n",
    "Pretty fun, huh? Even better, we can access much of this information via an API! Here's some [info from the Wayback Machine about how their API works](https://archive.org/help/wayback_api.php). Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b2202a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the packages we need\n",
    "\n",
    "# Lets us talk to other servers on the web\n",
    "import requests\n",
    "\n",
    "# APIs spit out data in JSON\n",
    "import json\n",
    "\n",
    "# Use BeautifulSoup to parse some HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Safetly quoting strings for URLs\n",
    "from urllib.parse import unquote, quote\n",
    "\n",
    "# Handling dates and times\n",
    "from datetime import datetime\n",
    "\n",
    "# DataFrames!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf8bf3",
   "metadata": {},
   "source": [
    "The simplest API request we can make asks for the most recent snapshot of a webpage archived by the Wayback Machine. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6014fd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'whitman.edu',\n",
       " 'archived_snapshots': {'closest': {'status': '200',\n",
       "   'available': True,\n",
       "   'url': 'http://web.archive.org/web/20231016153703/https://www.whitman.edu/',\n",
       "   'timestamp': '20231016153703'}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API calls come in the form of URLs\n",
    "wayback_url = 'http://archive.org/wayback/available?url=whitman.edu'\n",
    "\n",
    "# We can use requests.get() to get the contents of that URL\n",
    "wayback_response = requests.get(wayback_url)\n",
    "\n",
    "# Finally, we render the response as JSON using .json()\n",
    "wayback_response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b398b3",
   "metadata": {},
   "source": [
    "What do you notice about the response above? What information does it include? How is this information structured?'\n",
    "\n",
    "---\n",
    "__Fun note: APIs are just URLs!__ You don't need to write any code to check them out. Try pasting this URL, http://archive.org/wayback/available?url=whitman.edu, into your web browser. What happenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74c62ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://web.archive.org/web/20231016153703/https://www.whitman.edu/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task: Extract the URL from this request (this is the location of the page)\n",
    "# Save it as a variable called recent_whitman_url\n",
    "recent_whitman_url=wayback_response.json()\n",
    "recent_whitman_url['archived_snapshots']['closest']['url']\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "298c3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to build it up bit by bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "943e71f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for \"{'url': 'whitman.edu', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20231016153703/https://www.whitman.edu/', 'timestamp': '20231016153703'}}}\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# And now we can use requests.get() to grab the HTML\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m recent_whitman_response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecent_whitman_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Turn it into soup -- remember to use .text first!\u001b[39;00m\n\u001b[1;32m      5\u001b[0m recent_whitman_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(recent_whitman_response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:695\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m hooks \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mhooks\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Get the appropriate adapter to use\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m adapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# Start time (approximately) of the request\u001b[39;00m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:792\u001b[0m, in \u001b[0;36mSession.get_adapter\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m adapter\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# Nothing matches :-/\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidSchema(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo connection adapters were found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidSchema\u001b[0m: No connection adapters were found for \"{'url': 'whitman.edu', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20231016153703/https://www.whitman.edu/', 'timestamp': '20231016153703'}}}\""
     ]
    }
   ],
   "source": [
    "# And now we can use requests.get() to grab the HTML\n",
    "recent_whitman_response = requests.get(recent_whitman_url)\n",
    "\n",
    "# Turn it into soup -- remember to use .text first!\n",
    "recent_whitman_soup = BeautifulSoup(recent_whitman_response.text)\n",
    "\n",
    "# And find all the links\n",
    "links_text = [link.text for link in recent_whitman_soup.find_all('a')]\n",
    "\n",
    "# And print them out\n",
    "for l in links_text:\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cc66d",
   "metadata": {},
   "source": [
    "Ok, this is cool ... but it's much more fun to get HISTORICAL data. With the Wayback Machine API, we can also search for content around a given timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5333521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how we now have '&timestamp=20080201' in the URL\n",
    "# What do you think this means?\n",
    "wb_url = 'http://archive.org/wayback/available?url=whitman.edu&timestamp=20080201'\n",
    "\n",
    "# Use requests.get() to get the response\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "# Render it as JSON\n",
    "wb_response_json = wb_response.json()\n",
    "\n",
    "# And examine\n",
    "wb_response_json\n",
    "\n",
    "# What do you notice?\n",
    "# When was this page scraped by the Wayback Machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Make an API request to find out when Facebook's privacy policy (http://www.facebook.com/policy.php)\n",
    "# was archived in the Wayback Machine closest to January 1, 2008.\n",
    "url='http://archive.org/wayback/available?url=facebook.com/policy.php'\n",
    "# First construct the API url\n",
    "\n",
    "# Then use requests.get() to get the response\n",
    "fb_url=requests.get(url)\n",
    "# Turn it into JSON\n",
    "fb_json=fb_url.json()\n",
    "# And extract the timestamp\n",
    "fb_json['archived_snapshots']['closest']['timestamp']\n",
    "# What day was it archived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First construct the API url\n",
    "url = 'http://archive.org/wayback/available?url=facebook.com/policy.php&timestamp=20080101'\n",
    "\n",
    "# Then use requests.get() to get the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Turn it into JSON\n",
    "response_json = response.json()\n",
    "\n",
    "# And extract the timestamp\n",
    "response_json['archived_snapshots']['closest']['timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32d718",
   "metadata": {},
   "source": [
    "What might you do with this? For exmaple, you could examine how Facebook (or any company's) privacy policies or terms of service changed over time. This would be your starting point -- then you could compile the text and do some natural language processing to analyze it!\n",
    "\n",
    "A simple way to analyze how the privacy policies and terms of service have changed over time would be to see how the number of words has changed. Brian Keegan has an example of how to do this in his web scraping course -- I encourage you to [check it out!](https://github.com/CU-ITSS/Web-Data-Scraping-S2023/blob/main/Class%2004%20-%20Internet%20Archive%20and%20Wikipedia%20APIs/Class%2004%20-%20Scraping%20Internet%20Archive%20and%20Wikipedia.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "__Activity:__ Brainstorm with a partner for a few minutes about how you might use the Wayback Machine API to do a data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d8b06",
   "metadata": {},
   "source": [
    "### Using the TVMaze API\n",
    "\n",
    "Ok, let's try out a different API, this time from [TVMaze](https://www.tvmaze.com/api). This is an API that has information about tons and tons of TV shows.\n",
    "\n",
    "First let's make a basic request for information about a show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the basic URL -- we are going to build our query requests from this\n",
    "base_url = \"https://api.tvmaze.com\"\n",
    "\n",
    "# We can get information about specific shows by appending /show/ and then an ID number to the URL\n",
    "# The code below requests the info for show 321\n",
    "showInfo=requests.get(base_url +\"/shows/321\").json()\n",
    "print(showInfo)\n",
    "\n",
    "# What is the show name?\n",
    "\n",
    "# What information is included?\n",
    "\n",
    "# How is it structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3754379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would you print out the summary?\n",
    "showInfo['summary']\n",
    "# YOUR CODE HERE\n",
    "print(showInfo['summary'])\n",
    "# How would you print out the average rating?\n",
    "print(showInfo['rating']['average'])\n",
    "\n",
    "# YOUR CODE HEREabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ea05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "print(showInfo['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f27a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rating\n",
    "# note that we have a nested dictionary here!\n",
    "print(showInfo['rating']['average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b18e1",
   "metadata": {},
   "source": [
    "Ok, this is cool! But how do we know what shows are in the TVMaze database and what their IDs are?\n",
    "\n",
    "For this, we can do a [show search](https://www.tvmaze.com/api#show-search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60176678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by a string:\n",
    "showSearch = '/search/shows?q='\n",
    "queryString = 'bachelor'\n",
    "\n",
    "searchResults=requests.get(base_url + showSearch + queryString).json()\n",
    "print(searchResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3186d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK\n",
    "# What is the format of the results?\n",
    "# How many shows are in my results?\n",
    "# Print out all the names of the shows in the results\n",
    "# Print out all the IDs of the show results\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many shows are in the results?\n",
    "len(searchResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af918759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all the names\n",
    "for item in searchResults:\n",
    "    name = item['show']['name']\n",
    "    showID = item['show']['id']\n",
    "    print(\"Name: \" + item['show']['name'] + \", ID: \" + str(showID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa15b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could now use the show IDs to get the show info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d071d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Search for a different show\n",
    "# Based on the ID (or IDs) you get back, make an API request for the info TVMaze has about that show\n",
    "showquery= '/search/shows?q='\n",
    "queryString='Modern Family'\n",
    "searchResults=requests.get(base_url + showquery + queryString).json()\n",
    "print(searchResults)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069a313",
   "metadata": {},
   "source": [
    "These are just a few of the kinds of requests you can make using the TVMaze API.\n",
    "\n",
    "What other things can you do?\n",
    "\n",
    "**Acitivty:** Spend a few minutes lookat the the API's documentation. Try out a different kind of query. What did you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f47b89",
   "metadata": {},
   "source": [
    "### Suggestions for working with APIs\n",
    "\n",
    "1. Spend some time figuring out how they work...read the docs!\n",
    "2. The docs often have example queries. Use these to your advantage!\n",
    "3. Make some sample requests. Ask yourself: How is the data structured? Do I have any nested data structures?\n",
    "4. Often, you need to request preliminary information (like the show IDs above) in order to get the information you reallky want (the show facts above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbba61f",
   "metadata": {},
   "source": [
    "Let's explore! Pick an API to explore. You can use any that you want, but here are some you might consider:\n",
    "* [Pokemon API](https://pokeapi.co/)\n",
    "* [Dog API](https://dog.ceo/dog-api/) (pictures of dogs)\n",
    "* [SpaceX API](https://github.com/r-spacex/SpaceX-API/)\n",
    "* [COVID19 API](https://covid19api.com/)\n",
    "* [NASA APIs](https://api.nasa.gov/)\n",
    "* [EPA's Air Quality Index API](https://aqs.epa.gov/aqsweb/documents/data_api.html)\n",
    "* [Superhero API](https://superheroapi.com/?ref=apilist.fun)\n",
    "* [Open Movie Database](https://www.omdbapi.com/)\n",
    "* [New York Times](https://developer.nytimes.com/?ref=apilist.fun)\n",
    "* [Spoonaculur Food API](https://spoonacular.com/food-api)\n",
    "* [Open Library Books API](https://openlibrary.org/developers/api)\n",
    "\n",
    "\n",
    "Then, answer the following questions:\n",
    "\n",
    "1. Find the API's documentation. Spend some time reading about it -- what information does it have? How are queries structured? What kinds of different queries can you make?\n",
    "2. Try to make some intersting queries. What can you do?\n",
    "3. Think about how you might use this API to do a data science project. What questions might you be able to answer?\n",
    "4. How would you go about storing the data that you are getting back from the API?\n",
    "\n",
    "_Note: Some APIs require you to first request an API key. This is so you don't overload them with too many requests._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
